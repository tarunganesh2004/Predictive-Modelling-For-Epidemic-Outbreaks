{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Province/State Country/Region       Lat       Long  1/22/20  1/23/20  \\\n",
      "0            NaN    Afghanistan  33.93911  67.709953        0        0   \n",
      "1            NaN        Albania  41.15330  20.168300        0        0   \n",
      "2            NaN        Algeria  28.03390   1.659600        0        0   \n",
      "3            NaN        Andorra  42.50630   1.521800        0        0   \n",
      "4            NaN         Angola -11.20270  17.873900        0        0   \n",
      "\n",
      "   1/24/20  1/25/20  1/26/20  1/27/20  ...  2/28/23  3/1/23  3/2/23  3/3/23  \\\n",
      "0        0        0        0        0  ...   209322  209340  209358  209362   \n",
      "1        0        0        0        0  ...   334391  334408  334408  334427   \n",
      "2        0        0        0        0  ...   271441  271448  271463  271469   \n",
      "3        0        0        0        0  ...    47866   47875   47875   47875   \n",
      "4        0        0        0        0  ...   105255  105277  105277  105277   \n",
      "\n",
      "   3/4/23  3/5/23  3/6/23  3/7/23  3/8/23  3/9/23  \n",
      "0  209369  209390  209406  209436  209451  209451  \n",
      "1  334427  334427  334427  334427  334443  334457  \n",
      "2  271469  271477  271477  271490  271494  271496  \n",
      "3   47875   47875   47875   47875   47890   47890  \n",
      "4  105277  105277  105277  105277  105288  105288  \n",
      "\n",
      "[5 rows x 1147 columns]\n",
      "  Province/State Country/Region       Lat       Long  1/22/20  1/23/20  \\\n",
      "0            NaN    Afghanistan  33.93911  67.709953        0        0   \n",
      "1            NaN        Albania  41.15330  20.168300        0        0   \n",
      "2            NaN        Algeria  28.03390   1.659600        0        0   \n",
      "3            NaN        Andorra  42.50630   1.521800        0        0   \n",
      "4            NaN         Angola -11.20270  17.873900        0        0   \n",
      "\n",
      "   1/24/20  1/25/20  1/26/20  1/27/20  ...  2/28/23  3/1/23  3/2/23  3/3/23  \\\n",
      "0        0        0        0        0  ...     7896    7896    7896    7896   \n",
      "1        0        0        0        0  ...     3598    3598    3598    3598   \n",
      "2        0        0        0        0  ...     6881    6881    6881    6881   \n",
      "3        0        0        0        0  ...      165     165     165     165   \n",
      "4        0        0        0        0  ...     1933    1933    1933    1933   \n",
      "\n",
      "   3/4/23  3/5/23  3/6/23  3/7/23  3/8/23  3/9/23  \n",
      "0    7896    7896    7896    7896    7896    7896  \n",
      "1    3598    3598    3598    3598    3598    3598  \n",
      "2    6881    6881    6881    6881    6881    6881  \n",
      "3     165     165     165     165     165     165  \n",
      "4    1933    1933    1933    1933    1933    1933  \n",
      "\n",
      "[5 rows x 1147 columns]\n",
      "  Province/State Country/Region       Lat       Long  1/22/20  1/23/20  \\\n",
      "0            NaN    Afghanistan  33.93911  67.709953        0        0   \n",
      "1            NaN        Albania  41.15330  20.168300        0        0   \n",
      "2            NaN        Algeria  28.03390   1.659600        0        0   \n",
      "3            NaN        Andorra  42.50630   1.521800        0        0   \n",
      "4            NaN         Angola -11.20270  17.873900        0        0   \n",
      "\n",
      "   1/24/20  1/25/20  1/26/20  1/27/20  ...  2/28/23  3/1/23  3/2/23  3/3/23  \\\n",
      "0        0        0        0        0  ...        0       0       0       0   \n",
      "1        0        0        0        0  ...        0       0       0       0   \n",
      "2        0        0        0        0  ...        0       0       0       0   \n",
      "3        0        0        0        0  ...        0       0       0       0   \n",
      "4        0        0        0        0  ...        0       0       0       0   \n",
      "\n",
      "   3/4/23  3/5/23  3/6/23  3/7/23  3/8/23  3/9/23  \n",
      "0       0       0       0       0       0       0  \n",
      "1       0       0       0       0       0       0  \n",
      "2       0       0       0       0       0       0  \n",
      "3       0       0       0       0       0       0  \n",
      "4       0       0       0       0       0       0  \n",
      "\n",
      "[5 rows x 1147 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "confirmed_df = pd.read_csv(\"time_series_covid19_confirmed_global.csv\")\n",
    "deaths_df = pd.read_csv(\"time_series_covid19_deaths_global.csv\")\n",
    "recovered_df = pd.read_csv(\"time_series_covid19_recovered_global.csv\")\n",
    "\n",
    "# Inspect the data\n",
    "print(confirmed_df.head())\n",
    "print(deaths_df.head())\n",
    "print(recovered_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed DataFrame columns: Index(['Province/State', 'Country/Region', 'Lat', 'Long', 'Date', 'Confirmed'], dtype='object')\n",
      "Deaths DataFrame columns: Index(['Province/State', 'Country/Region', 'Lat', 'Long', 'Date', 'Deaths'], dtype='object')\n",
      "Recovered DataFrame columns: Index(['Province/State', 'Country/Region', 'Lat', 'Long', 'Date', 'Recovered'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocess_time_series(df, value_name):\n",
    "    # Melt the DataFrame to long format\n",
    "    df_melted = pd.melt(\n",
    "        df,\n",
    "        id_vars=[\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"],\n",
    "        var_name=\"Date\",\n",
    "        value_name=value_name,\n",
    "    )\n",
    "\n",
    "    # Convert 'Date' column to datetime\n",
    "    df_melted[\"Date\"] = pd.to_datetime(df_melted[\"Date\"], format=\"%m/%d/%y\")\n",
    "\n",
    "    # Sort by date\n",
    "    df_melted.sort_values(by=[\"Country/Region\", \"Province/State\", \"Date\"], inplace=True)\n",
    "\n",
    "    return df_melted\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "confirmed_df = pd.read_csv(\"time_series_covid19_confirmed_global.csv\")\n",
    "deaths_df = pd.read_csv(\"time_series_covid19_deaths_global.csv\")\n",
    "recovered_df = pd.read_csv(\"time_series_covid19_recovered_global.csv\")\n",
    "\n",
    "# Preprocess each DataFrame\n",
    "confirmed_df = preprocess_time_series(confirmed_df, \"Confirmed\")\n",
    "deaths_df = preprocess_time_series(deaths_df, \"Deaths\")\n",
    "recovered_df = preprocess_time_series(recovered_df, \"Recovered\")\n",
    "\n",
    "# Print columns to verify\n",
    "print(\"Confirmed DataFrame columns:\", confirmed_df.columns)\n",
    "print(\"Deaths DataFrame columns:\", deaths_df.columns)\n",
    "print(\"Recovered DataFrame columns:\", recovered_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Province/State Country/Region       Lat       Long       Date  Confirmed  \\\n",
      "0            NaN    Afghanistan  33.93911  67.709953 2020-01-22          0   \n",
      "1            NaN    Afghanistan  33.93911  67.709953 2020-01-23          0   \n",
      "2            NaN    Afghanistan  33.93911  67.709953 2020-01-24          0   \n",
      "3            NaN    Afghanistan  33.93911  67.709953 2020-01-25          0   \n",
      "4            NaN    Afghanistan  33.93911  67.709953 2020-01-26          0   \n",
      "\n",
      "   Deaths  Recovered  \n",
      "0       0        0.0  \n",
      "1       0        0.0  \n",
      "2       0        0.0  \n",
      "3       0        0.0  \n",
      "4       0        0.0  \n"
     ]
    }
   ],
   "source": [
    "def merge_in_chunks(df1, df2, chunk_size=5000):\n",
    "    merged_df = pd.DataFrame()\n",
    "    for start in range(0, len(df1), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = df1.iloc[start:end]\n",
    "        try:\n",
    "            chunk = pd.merge(\n",
    "                chunk,\n",
    "                df2,\n",
    "                on=[\"Province/State\", \"Country/Region\", \"Lat\", \"Long\", \"Date\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            merged_df = pd.concat([merged_df, chunk])\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}\")\n",
    "            print(f\"Columns in chunk: {chunk.columns}\")\n",
    "            print(f\"Columns in df2: {df2.columns}\")\n",
    "            break\n",
    "        except MemoryError:\n",
    "            print(f\"MemoryError at chunk {start}-{end}\")\n",
    "            break\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = merge_in_chunks(confirmed_df, deaths_df, chunk_size=5000)\n",
    "merged_df = merge_in_chunks(merged_df, recovered_df, chunk_size=5000)\n",
    "\n",
    "# Check the merged DataFrame\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "merged_df[\"Daily_Confirmed\"] = (\n",
    "    merged_df.groupby([\"Country/Region\", \"Province/State\"])[\"Confirmed\"]\n",
    "    .diff()\n",
    "    .fillna(0)\n",
    ")\n",
    "merged_df[\"Daily_Deaths\"] = (\n",
    "    merged_df.groupby([\"Country/Region\", \"Province/State\"])[\"Deaths\"].diff().fillna(0)\n",
    ")\n",
    "merged_df[\"Daily_Recovered\"] = (\n",
    "    merged_df.groupby([\"Country/Region\", \"Province/State\"])[\"Recovered\"]\n",
    "    .diff()\n",
    "    .fillna(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the main population data\n",
    "population_df = pd.read_csv(\n",
    "    \"API_SP.POP.TOTL_DS2_en_csv_v2_87.csv\", skiprows=4\n",
    ")  # Skipping the first few rows to get to the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Country Code                     Region          IncomeGroup  \\\n",
      "0          ABW  Latin America & Caribbean          High income   \n",
      "1          AFE                        NaN                  NaN   \n",
      "2          AFG                 South Asia           Low income   \n",
      "3          AFW                        NaN                  NaN   \n",
      "4          AGO         Sub-Saharan Africa  Lower middle income   \n",
      "\n",
      "                                        SpecialNotes  \\\n",
      "0                                                NaN   \n",
      "1  26 countries, stretching from the Red Sea in t...   \n",
      "2  The reporting period for national accounts dat...   \n",
      "3  22 countries, stretching from the westernmost ...   \n",
      "4  The World Bank systematically assesses the app...   \n",
      "\n",
      "                     TableName  Unnamed: 5  \n",
      "0                        Aruba         NaN  \n",
      "1  Africa Eastern and Southern         NaN  \n",
      "2                  Afghanistan         NaN  \n",
      "3   Africa Western and Central         NaN  \n",
      "4                       Angola         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load country metadata\n",
    "country_metadata_df = pd.read_csv(\n",
    "    \"Metadata_Country_API_SP.POP.TOTL_DS2_en_csv_v2_87.csv\"\n",
    "    \n",
    ")\n",
    "print(country_metadata_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Province/State Country/Region       Lat       Long       Date  Confirmed  \\\n",
      "0            NaN    Afghanistan  33.93911  67.709953 2020-01-22          0   \n",
      "1            NaN    Afghanistan  33.93911  67.709953 2020-01-23          0   \n",
      "2            NaN    Afghanistan  33.93911  67.709953 2020-01-24          0   \n",
      "3            NaN    Afghanistan  33.93911  67.709953 2020-01-25          0   \n",
      "4            NaN    Afghanistan  33.93911  67.709953 2020-01-26          0   \n",
      "\n",
      "   Deaths  Recovered  Daily_Confirmed  Daily_Deaths  Daily_Recovered  \\\n",
      "0       0        0.0              0.0           0.0              0.0   \n",
      "1       0        0.0              0.0           0.0              0.0   \n",
      "2       0        0.0              0.0           0.0              0.0   \n",
      "3       0        0.0              0.0           0.0              0.0   \n",
      "4       0        0.0              0.0           0.0              0.0   \n",
      "\n",
      "  Country Name  Population  \n",
      "0  Afghanistan  19542982.0  \n",
      "1  Afghanistan  19542982.0  \n",
      "2  Afghanistan  19542982.0  \n",
      "3  Afghanistan  19542982.0  \n",
      "4  Afghanistan  19542982.0  \n"
     ]
    }
   ],
   "source": [
    "# Clean column names\n",
    "population_df.columns = population_df.columns.str.strip()\n",
    "\n",
    "# Merge on the country name or country code\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    population_df[[\"Country Name\", \"2000\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"Country/Region\",\n",
    "    right_on=\"Country Name\",\n",
    ")\n",
    "\n",
    "# Rename columns for clarity if needed\n",
    "merged_df.rename(columns={\"2000\": \"Population\"}, inplace=True)\n",
    "\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"merged_df_population.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_with_population = pd.read_csv(\"merged_df_population.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Province/State', 'Country/Region', 'Lat', 'Long', 'Date', 'Confirmed',\n",
       "       'Deaths', 'Recovered', 'Daily_Confirmed', 'Daily_Deaths',\n",
       "       'Daily_Recovered', 'Country Name', 'Population'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_with_population.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hotspot\n",
      "False    316879\n",
      "True      13448\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define thresholds\n",
    "infection_percent_threshold = 0.1  # Set to 0.1% for now\n",
    "daily_cases_threshold = 100  # Set an absolute threshold for daily cases\n",
    "rolling_window = 7  # 7-day rolling window to smooth the data\n",
    "\n",
    "# Calculate infection rate\n",
    "merged_df_with_population[\"Infection_Rate\"] = (\n",
    "    merged_df_with_population[\"Daily_Confirmed\"]\n",
    "    / merged_df_with_population[\"Population\"]\n",
    ") * 100\n",
    "\n",
    "# Apply rolling averages for smoothing\n",
    "merged_df_with_population[\"Rolling_Daily_Confirmed\"] = (\n",
    "    merged_df_with_population[\"Daily_Confirmed\"].rolling(window=rolling_window).mean()\n",
    ")\n",
    "merged_df_with_population[\"Rolling_Infection_Rate\"] = (\n",
    "    merged_df_with_population[\"Infection_Rate\"].rolling(window=rolling_window).mean()\n",
    ")\n",
    "\n",
    "# Hotspot is True if either the infection rate or the absolute daily cases exceed the thresholds\n",
    "merged_df_with_population[\"Hotspot\"] = (\n",
    "    merged_df_with_population[\"Rolling_Infection_Rate\"] >= infection_percent_threshold\n",
    ") | (merged_df_with_population[\"Rolling_Daily_Confirmed\"] >= daily_cases_threshold)\n",
    "\n",
    "# Display the results\n",
    "print(merged_df_with_population[\"Hotspot\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Province/State', 'Country/Region', 'Lat', 'Long', 'Date', 'Confirmed',\n",
      "       'Deaths', 'Recovered', 'Daily_Confirmed', 'Daily_Deaths',\n",
      "       'Daily_Recovered', 'Country Name', 'Population', 'Infection_Rate',\n",
      "       'Rolling_Daily_Confirmed', 'Rolling_Infection_Rate', 'Hotspot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_df_with_population.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\emada\\anaconda3\\lib\\site-packages (1.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib  # For saving the model\n",
    "\n",
    "# Load your dataset\n",
    "# merged_df_with_population = pd.read_csv('path_to_your_merged_data.csv')\n",
    "\n",
    "# Select features and target\n",
    "features = [\"Daily_Confirmed\", \"Infection_Rate\", \"Rolling_Daily_Confirmed\"]\n",
    "target = \"Hotspot\"\n",
    "\n",
    "# Drop rows with NaN values in selected columns\n",
    "df_model = merged_df_with_population.dropna(subset=features + [target])\n",
    "\n",
    "# Define X and y\n",
    "X = df_model[features]\n",
    "y = df_model[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model and scaler for use in the Flask app\n",
    "joblib.dump(knn, \"knn_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
